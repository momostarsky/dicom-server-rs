-- we don't know how to generate root <with-no-name> (class Root) :(

create table dicom_image_meta
(
    tenant_id                  String,
    patient_id                 String,
    study_uid                  String,
    series_uid                 String,
    sop_uid                    String,
    study_uid_hash             String,
    series_uid_hash            String,
    study_date_origin          Date,
    content_date               Nullable(Date),
    content_time               Nullable(String),
    instance_number            Nullable(Int32),
    image_type                 Nullable(String),
    image_orientation_patient  Nullable(String),
    image_position_patient     Nullable(String),
    slice_thickness            Nullable(Float64),
    spacing_between_slices     Nullable(Float64),
    slice_location             Nullable(Float64),
    samples_per_pixel          Nullable(Int32),
    photometric_interpretation Nullable(String),
    width                      Nullable(Int32),
    columns                    Nullable(Int32),
    bits_allocated             Nullable(Int32),
    bits_stored                Nullable(Int32),
    high_bit                   Nullable(Int32),
    pixel_representation       Nullable(Int32),
    rescale_intercept          Nullable(Float64),
    rescale_slope              Nullable(Float64),
    rescale_type               Nullable(String),
    window_center              Nullable(String),
    window_width               Nullable(String),
    transfer_syntax_uid        String,
    pixel_data_location        Nullable(String),
    thumbnail_location         Nullable(String),
    sop_class_uid              String,
    image_status               Nullable(String),
    space_size                 Nullable(Int64),
    created_time               Nullable(DateTime),
    updated_time               Nullable(DateTime)
)
    engine = MergeTree ORDER BY (tenant_id, patient_id, study_uid, series_uid, sop_uid)
        SETTINGS index_granularity = 8192;

create table dicom_image_meta_kafka
(
    tenant_id                  String,
    patient_id                 String,
    study_uid                  String,
    series_uid                 String,
    sop_uid                    String,
    study_uid_hash             String,
    series_uid_hash            String,
    study_date_origin          Date,
    content_date               Nullable(Date),
    content_time               Nullable(String),
    instance_number            Nullable(Int32),
    image_type                 Nullable(String),
    image_orientation_patient  Nullable(String),
    image_position_patient     Nullable(String),
    slice_thickness            Nullable(Float64),
    spacing_between_slices     Nullable(Float64),
    slice_location             Nullable(Float64),
    samples_per_pixel          Nullable(Int32),
    photometric_interpretation Nullable(String),
    width                      Nullable(Int32),
    columns                    Nullable(Int32),
    bits_allocated             Nullable(Int32),
    bits_stored                Nullable(Int32),
    high_bit                   Nullable(Int32),
    pixel_representation       Nullable(Int32),
    rescale_intercept          Nullable(Float64),
    rescale_slope              Nullable(Float64),
    rescale_type               Nullable(String),
    window_center              Nullable(String),
    window_width               Nullable(String),
    transfer_syntax_uid        String,
    pixel_data_location        Nullable(String),
    thumbnail_location         Nullable(String),
    sop_class_uid              String,
    image_status               Nullable(String),
    space_size                 Nullable(Int64),
    created_time               Nullable(String),
    updated_time               Nullable(String)
)
    engine = Kafka SETTINGS kafka_broker_list = 'redpanda:9092', kafka_topic_list = 'dicom_image_queue', kafka_group_name = 'medical_image_group', kafka_format = 'JSONEachRow', kafka_max_block_size = 1048576, kafka_skip_broken_messages = 1;

create table dicom_object_meta
(
    tenant_id           String,
    patient_id          String,
    study_uid           String,
    series_uid          String,
    sop_uid             String,
    file_size           Nullable(Int64),
    file_path           Nullable(String),
    transfer_syntax_uid Nullable(String),
    number_of_frames    Nullable(Int32),
    created_time        Nullable(DateTime),
    series_uid_hash     Nullable(String),
    study_uid_hash      Nullable(String),
    accession_number    Nullable(String),
    target_ts           Nullable(String),
    study_date          Nullable(Date),
    transfer_status     Nullable(String),
    source_ip           Nullable(String),
    source_ae           Nullable(String),
    trace_id            String,
    worker_node_id      String
)
    engine = MergeTree ORDER BY (tenant_id, patient_id, study_uid, series_uid, sop_uid)
        SETTINGS index_granularity = 8192;

create table dicom_object_meta_kafka
(
    tenant_id           String,
    patient_id          String,
    study_uid           String,
    series_uid          String,
    sop_uid             String,
    file_size           Nullable(Int64),
    file_path           Nullable(String),
    transfer_syntax_uid Nullable(String),
    number_of_frames    Nullable(Int32),
    created_time        Nullable(String),
    series_uid_hash     Nullable(String),
    study_uid_hash      Nullable(String),
    accession_number    Nullable(String),
    target_ts           Nullable(String),
    study_date          Nullable(Date),
    transfer_status     Nullable(String),
    source_ip           Nullable(String),
    source_ae           Nullable(String),
    trace_id            String,
    worker_node_id      String
)
    engine = Kafka SETTINGS kafka_broker_list = 'redpanda:9092', kafka_topic_list = 'log_queue', kafka_group_name = 'medical_object_group', kafka_format = 'JSONEachRow', kafka_max_block_size = 1048576, kafka_skip_broken_messages = 1;

create table dicom_state_meta
(
    tenant_id          String,
    patient_id         String,
    study_uid          String,
    series_uid         String,
    study_uid_hash     String,
    series_uid_hash    String,
    study_date_origin  String,
    patient_name       Nullable(String),
    patient_sex        Nullable(String),
    patient_birth_date Nullable(Date),
    patient_birth_time Nullable(String),
    patient_age        Nullable(String),
    patient_size       Nullable(Float64),
    patient_weight     Nullable(Float64),
    pregnancy_status   Nullable(Int32),
    study_date         Date,
    study_time         Nullable(String),
    accession_number   String,
    study_id           Nullable(String),
    study_description  Nullable(String),
    modality           Nullable(String),
    series_number      Nullable(Int32),
    series_date        Nullable(Date),
    series_time        Nullable(String),
    series_description Nullable(String),
    body_part_examined Nullable(String),
    protocol_name      Nullable(String),
    created_time       Nullable(DateTime),
    updated_time       Nullable(DateTime)
)
    engine = MergeTree ORDER BY (tenant_id, patient_id, study_uid, series_uid)
        SETTINGS index_granularity = 8192;

create table dicom_state_meta_kafka
(
    tenant_id          String,
    patient_id         String,
    study_uid          String,
    series_uid         String,
    study_uid_hash     String,
    series_uid_hash    String,
    study_date_origin  String,
    patient_name       Nullable(String),
    patient_sex        Nullable(String),
    patient_birth_date Nullable(Date),
    patient_birth_time Nullable(String),
    patient_age        Nullable(String),
    patient_size       Nullable(Float64),
    patient_weight     Nullable(Float64),
    pregnancy_status   Nullable(Int32),
    study_date         Date,
    study_time         Nullable(String),
    accession_number   String,
    study_id           Nullable(String),
    study_description  Nullable(String),
    modality           Nullable(String),
    series_number      Nullable(Int32),
    series_date        Nullable(Date),
    series_time        Nullable(String),
    series_description Nullable(String),
    body_part_examined Nullable(String),
    protocol_name      Nullable(String),
    created_time       Nullable(String),
    updated_time       Nullable(String)
)
    engine = Kafka SETTINGS kafka_broker_list = 'redpanda:9092', kafka_topic_list = 'dicom_state_queue', kafka_group_name = 'medical_state_group', kafka_format = 'JSONEachRow', kafka_max_block_size = 1048576, kafka_skip_broken_messages = 1;

CREATE MATERIALIZED VIEW default.dicom_image_meta_mv
            TO default.dicom_image_meta
            (
            `tenant_id` String,
            `patient_id` String,
            `study_uid` String,
            `series_uid` String,
            `sop_uid` String,
            `study_uid_hash` String,
            `series_uid_hash` String,
            `study_date_origin` Date,
            `content_date` Nullable(Date),
            `content_time` Nullable(String),
            `instance_number` Nullable(Int32),
            `image_type` Nullable(String),
            `image_orientation_patient` Nullable(String),
            `image_position_patient` Nullable(String),
            `slice_thickness` Nullable(Float64),
            `spacing_between_slices` Nullable(Float64),
            `slice_location` Nullable(Float64),
            `samples_per_pixel` Nullable(Int32),
            `photometric_interpretation` Nullable(String),
            `width` Nullable(Int32),
            `columns` Nullable(Int32),
            `bits_allocated` Nullable(Int32),
            `bits_stored` Nullable(Int32),
            `high_bit` Nullable(Int32),
            `pixel_representation` Nullable(Int32),
            `rescale_intercept` Nullable(Float64),
            `rescale_slope` Nullable(Float64),
            `rescale_type` Nullable(String),
            `window_center` Nullable(String),
            `window_width` Nullable(String),
            `transfer_syntax_uid` String,
            `pixel_data_location` Nullable(String),
            `thumbnail_location` Nullable(String),
            `sop_class_uid` String,
            `image_status` Nullable(String),
            `space_size` Nullable(Int64),
            `created_time` Nullable(DateTime),
            `updated_time` Nullable(DateTime)
            )
AS
SELECT tenant_id,
       patient_id,
       study_uid,
       series_uid,
       sop_uid,
       study_uid_hash,
       series_uid_hash,
       study_date_origin,
       content_date,
       content_time,
       instance_number,
       image_type,
       image_orientation_patient,
       image_position_patient,
       slice_thickness,
       spacing_between_slices,
       slice_location,
       samples_per_pixel,
       photometric_interpretation,
       width,
       columns,
       bits_allocated,
       bits_stored,
       high_bit,
       pixel_representation,
       rescale_intercept,
       rescale_slope,
       rescale_type,
       window_center,
       window_width,
       transfer_syntax_uid,
       pixel_data_location,
       thumbnail_location,
       sop_class_uid,
       image_status,
       space_size,
       parseDateTimeBestEffortOrNull(created_time) AS created_time,
       parseDateTimeBestEffortOrNull(updated_time) AS updated_time
FROM default.dicom_image_meta_kafka;

CREATE MATERIALIZED VIEW default.dicom_object_meta_mv
            TO default.dicom_object_meta
            (
            `tenant_id` String,
            `patient_id` String,
            `study_uid` String,
            `series_uid` String,
            `sop_uid` String,
            `file_size` Nullable(Int64),
            `file_path` Nullable(String),
            `transfer_syntax_uid` Nullable(String),
            `number_of_frames` Nullable(Int32),
            `created_time` Nullable(DateTime),
            `series_uid_hash` Nullable(String),
            `study_uid_hash` Nullable(String),
            `accession_number` Nullable(String),
            `target_ts` Nullable(String),
            `study_date` Nullable(Date),
            `transfer_status` Nullable(String),
            `source_ip` Nullable(String),
            `source_ae` Nullable(String),
            `trace_id` String,
            `worker_node_id` String
            )
AS
SELECT tenant_id,
       patient_id,
       study_uid,
       series_uid,
       sop_uid,
       file_size,
       file_path,
       transfer_syntax_uid,
       number_of_frames,
       parseDateTimeBestEffortOrNull(created_time) AS created_time,
       series_uid_hash,
       study_uid_hash,
       accession_number,
       target_ts,
       study_date,
       transfer_status,
       source_ip,
       source_ae,
       trace_id,
       worker_node_id
FROM default.dicom_object_meta_kafka;

CREATE MATERIALIZED VIEW default.dicom_state_meta_mv
            TO default.dicom_state_meta
            (
            `tenant_id` String,
            `patient_id` String,
            `study_uid` String,
            `series_uid` String,
            `study_uid_hash` String,
            `series_uid_hash` String,
            `study_date_origin` String,
            `patient_name` Nullable(String),
            `patient_sex` Nullable(String),
            `patient_birth_date` Nullable(Date),
            `patient_birth_time` Nullable(String),
            `patient_age` Nullable(String),
            `patient_size` Nullable(Float64),
            `patient_weight` Nullable(Float64),
            `pregnancy_status` Nullable(Int32),
            `study_date` Date,
            `study_time` Nullable(String),
            `accession_number` String,
            `study_id` Nullable(String),
            `study_description` Nullable(String),
            `modality` Nullable(String),
            `series_number` Nullable(Int32),
            `series_date` Nullable(Date),
            `series_time` Nullable(String),
            `series_description` Nullable(String),
            `body_part_examined` Nullable(String),
            `protocol_name` Nullable(String),
            `created_time` Nullable(DateTime),
            `updated_time` Nullable(DateTime)
            )
AS
SELECT tenant_id,
       patient_id,
       study_uid,
       series_uid,
       study_uid_hash,
       series_uid_hash,
       study_date_origin,
       patient_name,
       patient_sex,
       patient_birth_date,
       patient_birth_time,
       patient_age,
       patient_size,
       patient_weight,
       pregnancy_status,
       study_date,
       study_time,
       accession_number,
       study_id,
       study_description,
       modality,
       series_number,
       series_date,
       series_time,
       series_description,
       body_part_examined,
       protocol_name,
       parseDateTimeBestEffortOrNull(created_time) AS created_time,
       parseDateTimeBestEffortOrNull(updated_time) AS updated_time
FROM default.dicom_state_meta_kafka;

